---
title: "<FONT color='#0066CC'><FONT size = 4 ><DIV align= center> AP-4209 ESIEE-Paris: 2023 -2024 </DIV></FONT></FONT>"
output:
  html_document:
    highlight: textmate
    theme: readable
    toc: yes
    toc_depth: 6
    toc_float: yes
---


<style type="text/css">
body, td {font-size: 15px;}
code.r{font-size: 5px;}
pre { font-size: 12px;}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<FONT color='#0066CC'><FONT size = 4 >

::: {align="center"}
Fouille de données avec R pour la data science et l'intelligence artificielle\

III.TD 3 : Partie II - ANALYSE FACTORIELLE DISCRIMINANTE
:::

</FONT></FONT>

<FONT color='#0066CC'><FONT size = 4 >

::: {align="center"}
Badr TAJINI -- ESIEE Paris\
Source : Bertrand Roudier -- ESIEE Paris
:::

</FONT></FONT>

<hr style="border: 1px  solid gray">

</hr>

<DIV align = justify>

### <FONT color='#0066CC'><FONT size = 4> 1. Introduction </FONT></FONT>

Ce TD a pour objectif de réaliser une analyse factorielle discriminante nous permettant : 

 * à l'aide d'une diminution de dimentionnalité, de visualiser les données dans le plan des axes facoriels (cf.cours)
 * d'effectuer, pour chaque axe, une inférence statistique qui permet de tester la discrimination des différentes classes projetées. 
 
 Au total, l'AFD  est analogue à une MANOVA, nous permettant de réaliser, de manière concomitante, une visualisation des données.
 
 Pour y parvenir, nous utiliserons le jeu de données *VIN_QUALITE.txt*
 
<br>

<hr style="border: 1px  solid gray">

### <FONT color='#0066CC'><FONT size = 4> 2. Prés-Requis </FONT></FONT>

* Nous chargeons le fichier *VIN_QUALITE.txt*
* Nous utilisons la fonction que nous avons développée lors du précédant TP pour calculer les sommes des carrés totaux, inter et intra.

<br>

```{r, echo = T, warning=F, message=F}
df <- read.table('VIN_QUALITE.txt', header=TRUE)
head(df)
```

```{r}
# à compléter
```


```{r}

# à compléter

```

Les résultats sont les suivants:  

* La somme des carrés totaux :  *SS <SUB>Tot</SUB>*

```{r}
# à compléter
```

<br> 

* La somme des carrés intra : *SS <SUB>Intra</SUB>*

```{r}
# à compléter
```


<br> 

* La somme des carrés inter : *SS <SUB>Inter</SUB>*

```{r}
# à compléter
```


<br>

<hr style="border: 1px  solid gray">

### <FONT color='#0066CC'><FONT size = 4> 3. Analyse Factorielle Discriminante </FONT></FONT>

#### <FONT color='#0066CC'><FONT size = 4> 3.1 Rappels </FONT></FONT>

Comme nous l'avons vu en cours, l'analyse factorielle discriminante consiste à trouver une succession d'axes factoriels, tous orthogonaux entre eux et qui maximisent les projections des distances entre les groupes (cf. schéma suivant pour rappel)


```{r, echo=FALSE, fig.width = 4, fig.height = 4, fig.align = 'center'}
knitr::include_graphics('AFD_Axe_Facto.jpg')
```

<br> 

Maximiser les distances entre les groupes revient à maximiser les projections suivantes :

*  ${P_1} = {\max _u}\left( {\frac{{{u^t}Bu}}{{{u^t}Wu}}} \right)$ (école Anglo saxonne)

ou bien 

* ${P_2} = {\max _u}\left( {\frac{{{u^t}Bu}}{{{u^t}Wu}}} \right)$ (école Française)

<br>

Fort heureusement, les deux méthodes conduisent aux mêmes résultats. Cependant, les approches sont légèrement différentes. 

* La méthode anglo-saxonne à un raisonnement analogue à la construction du test d'analyse de variance ou l'on teste le rapport Signal / Bruit (cf. cours page 11). Dans ce cadre, la projection des distances inter groupes est pondérée par les distances intra-groupes

* La méthode française privilégie quant à elle la corrélation canonique c.a.d la part de variation liés au traitement (cf. cours page 11). Dans ce cadre, la projection des distances inter groupes est pondérée par la variation totale 

Vous réaliserez une fonction permettant de réaliser une AFD selon la projection *P<SUB>1</SUB>* (méthode anglo saxonne). Pour y parvenir, nous allons décrire les différentes étapes avec les résultats intermédiaires.  

<br> 

<!---////////////////////////////////////////////////////////////////////////////--->
#### <FONT color='#0066CC'><FONT size = 4> 3.2 Diagonalisation </FONT></FONT>

<br>

* Nous calculons la matrice du ratio :  


  $\frac{B}{W} = \frac{{S{S_{{\rm{inter}}}}}}{{S{S_{{\rm{intra}}}}}} = B \times {W^{ - 1}} = S{S_{{\rm{inter}}}} \times SS_{{\rm{intra}}}^{ - 1}$ 


Attention la fraction ici correspond à une *division matricielle* et non à une division élément par élément !

Le résultat est le suivant : 
```{r}
# à compléter
```

<br>

Nous calculons maintenant les vecteurs directeurs *u* des axes factoriels et leur coefficient.
De manière analogue à l'ACP, la maximisation de \[{P_1} = {\max _u}\left( {\frac{{{u^t}Bu}}{{{u^t}Wu}}} \right)\]
revient à diagonaliser la matrice $S{S_{{\rm{inter}}}} \times SS_{{\rm{intra}}}^{ - 1}$. Les vecteurs propres correspondent alors aux vecteurs directeurs des axes factoriels. Ces derniers devront être cependant normalisés.   
Comme en ACP, les valeurs propres correspondent à la part de dispersion expliquée par chaque axe.  

 * La diagonalisation est réalisée à l'aide de la fonction *eigen()*

 * *<U> remarque importante </U>* : Les algorithmes utilisés peuvent conduire à des matrices de vecteurs propres dont les éléments sont des complexes. *On ne retiendra donc que les parties réels en éliminant les parties complexes* (utiliser la fonction *Re*)

<br> 


##### <FONT color='#0066CC'><FONT size = 4> 3.2.1 Vecteurs directeurs </FONT></FONT>

* Matrice des vecteurs propres *U*

```{r}
# à compléter
```


<br>

Contrairement à l'ACP où nous diagonalisons une matrice symétrique (matrice des variances covariances ou de corrélation), la matrice du ratio *B/W* n'est pas symétrique ce qui conduit à des résultats non exploitables car non normées. Pour y parvenir, nous normalisons la matrice des vecteurs propres :

* Soit *U* la matrice des vecteurs propres: nous calculons *U <SUB> Norm </SUB>* tel que :

 <DIV align = 'center'> ${U_{Norm}} = \frac{U}{{\sqrt {diag({U^t} \times W \times U)} }}$ </DIV>

<br> 

* la matrice de normalisation (dénominateur) est la suivante :

```{r}

# à compléter
```


<br>

* La matrice des vecteurs propres normalisées *Un* est:

```{r}
# à compléter
```


<br> 

##### <FONT color='#0066CC'><FONT size = 4> 3.2.2  Valeurs propres </FONT></FONT>

  * Comme pour les vecteurs propres, nous éliminons la partie imaginaire. les valeurs sont les suivantes:    

<br>


```{r}
# à compléter
``` 

  * De manière analogue à l'ACP, les valeurs propres correspondent à la variance expliquée par les axes. Les variances expliquées par les deux derniers axes sont égales à 0. Dans certains cas, les valeur propres peuvent être négatives (ce qui incohérent car il s'agit de variances!) et de très faible valeur (inférieures à 10-7). *Il s'agit d'une erreur de virgule flottante imputable aux calculs itératifs nécessaires à l'estimation des valeurs propres*. Dans ce contexte, on considère ces valeurs comme nulles.  

<br>

* la part de dispersion expliquée par les axes (inertie des axes) est calculée par l'expression suivante:  

<DIV align = center>  ${I_i} = \frac{{{\lambda _i}}}{{\sum\limits_i {{\lambda _i}} }}$  </DIV>

Les parts de dispersion sont les suivantes :

```{r}
# à compléter
```


<br>

Comme on peut le constater, le premier axe explique à lui seul prés de 96 % de la dispersion et le second 4% !. 

<br>


<!--///////////////////////////////////////////////////////////////////////////////////////////////-->
#### <FONT color='#0066CC'><FONT size = 4> 3.3 Coordonnées des individus </FONT></FONT>
<!--///////////////////////////////////////////////////////////////////////////////////////////////-->

Le calcul des coordonnées des individus sur les axes factoriels (appelées *scores* en anglais) s'effectue en réalisant la projection des observations X **(centrées)** sur les axes factoriels. 
Les scores sont simplement calculés comme suit :

$Score = Z \times {U_{Norm}}$  

Sachant que pour chaque variable *i* de *X* avec ${Z_j} = {X_j} - {{\bar X}_j}$, il suffit donc de multiplier les données centrées *Z* par *Unorm*

Seuls les deux premiers axes factoriels sont à prendre en compte puisqu'ils représentent à eux seul l'intégralité de la dispersion et donc de "l'information contenu dans le tableau initial *Z*".

Nous créons un data.frame *Scores_df* qui inclue les coordonnées des individus sur les deux premiers axes factoriels ainsi que les différentes classes auxquelles ils appartiennent (variable 'Class'). On prendra soin de bien vérifier que l'entête de la variable (nom de la variable) soit égale à *'Class'*

<br>

```{r, echo = F}
# à compléter
```


```{r, echo = F}
# à compléter
```

<br> 

Nous utilisons la fonction *AFD_graph1*  (fournie ci dessous) pour représenter les individus dans le plan factoriel. Les couleurs permettent de différentier les classes. les losanges représentent le centre de gravité des différentes classes

<br>

```{r, echo = T}
# à compléter

```

```{r}
# à compléter
```



<br>

#### <FONT color='#0066CC'><FONT size = 4> 3.4 Inférence statistique </FONT></FONT>

Nous effectuons un test de Wilks. En analyse factorielle discriminante, il s'agit de tester successivement le caractère discriminant des axes vis à vis des différentes classes.

<br> 

Pour y parvenir, nous calculons les corrélations canoniques de chaque axe. Soit $\rho$, la valeur propre associée à chaque axe, la corrélation canonique est :
  
 \[{\eta ^2} = \frac{\rho }{{1 + \rho }}\]

les corrélations canoniques des deux premiers axes sont:  
```{r}
# à compléter
```

Dans ce TD, nous allons simplifier les hypothèses. Sous Ho, nous posons que les corrélations canoniques des axes factoriels retenus sont égales à zero *versus* H1: au moins une des corrélations différent.  

Le test est le suivant:  

<br>

\[\left\{ \begin{array}{l}
{H_0}:\eta _{{\rm{axe 1}}}^2 = \eta _{{\rm{axe 2}}}^2 = 0\\
{H_1}:{\rm{ }}\eta _{{\rm{axe 1}}}^2{\rm{ et / ou  }}\eta _{{\rm{axe 2}}}^2 \ne 0
\end{array} \right.\]

<br>

Nous calculons la quantité de Wilks $Wilks = {\Lambda _i} = \prod\limits_{i = 1}^2 {(1 - \lambda _i^2)}$

  * rmq : Pour la calculer, on utilisera la fonction *cumsum()*

<br> 

La quantité suivante suit une distribution de Chi-deux

\[ - \left( {n - 1 - \frac{{p + k}}{2}} \right)\log (\Lambda ) \to \chi _{p(k - 1)ddl}^2\]  

avec:

  * n: le nombre total d'observations  
  * p: le nombre de variables
  * k: le nombre de classes

<br> 

Pour les deux premiers axes retenus *(axe_selected = 2)*, L'ensemble des résultats de l'ADF sont résumés dans le tableau :

```{r}
# à compléter

```

<br>

Les résultats précédents montrent que les  deux axes discriminent parfaitement les classes (les probas associées à la discrimination sur chaque axe étant proche de zéro). *Attention cela ne prédispose pas de la classification de chaque individus dans les différentes groupes* (ce que nous verrons au prochains TD (TD final)).

### <FONT color='#0066CC'><FONT size = 4> 4. Encapsulation du code </FONT></FONT>

Pour rappel, l'AFD est une méthode de classification supervisée. Le présent TD nous a permis de développer des scripts permettant:   

<br>

  * De calculer les axes factoriels relatives aux projections du compromis B/W  
  
  * De positionner les individus dans le plan factoriel ce qui permet de visualiser les positions des individus et des groupes les uns par rapport aux autres  
  
  * De tester la qualité de la projection et de la discrimination des groupes sur les axes factoriels    

<br>

Dans notre exemple, la qualité de discrimination des différents groupes selon les axes est excellente.

Le prochain TD aura pour objectif d'utiliser les coordonnées des observations dans le plan factoriel pour réaliser un classifieur supervisée dont nous testerons la qualité à l'aide d'une matrice des confusions.

<br>

Remarque :  En cas de *non* discrimination des classes par les axes factoriels (acceptation de Ho et rejet de  H1), il est évident qu'il n'est pas possible d'utiliser les projections des individus sur les axes factoriels pour réaliser un classifieur. Dans ce cas, la classification supervisée n'est pas réalisable par cette méthode

Nous créons une fonction générique que  nous nommons *AFD*. Les arguments de la fonction sont les suivants : 

  * AFD <- function(X,Y,SS_tot, SS_intra, SS_inter, nb_axes = 2)  
    
    - X et Y sont respectivement les variables prédictives et Y la variable à prédire    
    
    - SS_tot, SS_intra, SS_inter sont les sommes des carrés calculées à partir de la fonction *MANOVA* que vous avez développée  
    
    - nb_axes est le nombre d'axes sélectionné pour réaliser les projections. Par défaut, il est égal à 2 (projection dans un plan)  
    
Cette fonction retourne une liste dont les éléments sont les suivants (sous forme de data frame):  
  
  * les vecteurs propres normalisés (appelés aussi *loading factors* en anglais)  
  
  * les valeur propres  
  
  * les scores    
  
  * le tableau des résultats du test de Wilks  

Le data frame des Scores calculés à partir da la fonction *AFD* corresppondent à l'argument de la fonction *AFD_graph1*

Le script final (avec les résultats) doit être le suivant :

```{r}
# à compléter

```


```{r, echo = T}
# à compléter

```


### <FONT color='#0066CC'><FONT size = 4> 5. Application </FONT></FONT>

Pour valider votre fonction, vous utiliserez le fichier iris fourni par défaut en R.

Les résultats sont les suivants :

```{r}
# à compléter

```


* Matrice des vecteurs propres normalisées  

```{r}
# à compléter
```

<br> 

* Vecteurs propres  

```{r}
# à compléter
```

<br>

* Scores  

```{r}
# à compléter
```

<br>

* Tests MANOVA axes factoriels  

```{r}
# à compléter
```


* Si vous trouvez les mêmes résultats c'est que vous avez bien travaillé !

