---
title: "Classification Bayésienne"
output:
  html_document:
    theme: readable
    toc: yes
    toc_depth: 6
    toc_float: yes
author: "Paul Cascarino et Mathis Quinio-Cosquer"
date: "Décembre 2023"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Projet 1 : 

## 1. Introduction

### Les objectifs du projet
Le but de ce projet est de faire de la recherche dans le 
traitement du langage naturel et des émotions. Ainsi le principal
objectif sera de faire de la prédiction sur les émotions à partir
de textes.


## 2. Chargement et exploration des données

### Préparation et imports
Dans un premier temps, nous effaçons la mémoire : 
```{r}
rm(list=ls())
```

Et faisons les chargements des packages nécessaires : 
```{r}
library(kableExtra)
library(e1071)
library(caret)
library(ggplot2)
library(dplyr)
library(stringr)

# Permet la tokenization : https://smltar.com/tokenization#types-of-tokens 
library(tokenizers)

# Permet l'utilisation de liste pour le stop-words : https://smltar.com/stopwords#premadestopwords
library(stopwords)

# Permet la stemmatization : https://smltar.com/stemming#how-to-stem-text-in-r
library(SnowballC)

#library(spacyr)
library(tm)
library(text2vec)
library(slam)

library(quanteda)
library(quanteda.textmodels)
```

__Explication :__

1. `library(kableExtra)` charge le package `kableExtra` qui est utilisé pour
  créer des tables formatées
2. `library(e1071)` et `library(caret)` : Charge les packages `e1071` et `caret`. 
  `e1071` est souvent utilisé pour des méthodes statistiques, 
  et `caret` pour le machine learning.
3. `library(ggplot2)` charge le package `ggplot2` qui permet de créer 
  facilement des graphiques en R
4. `library(dplyr)` charge le package `dplyr` qui offre des fonctions
  utiles et simples dans la manipulation de données
5. `library(stringr)` charge le package `stringr` qui offre des fonctions
  utiles dans la manipulation des chaînes de caractères



### Chargement des données
Les données sont récupérés sur le site kaggle à l'adresse : <https://www.kaggle.com/datasets/abdallahwagih/emotion-dataset>
Nous pouvons ainsi récupérer le csv récupéré avec : 
```{r}
df <- read.csv('data/Emotion_classify_Data.csv')
#df <- df[0:500,]
```
Voici un aperçu de nos données : 
```{r}
head(df)
```

### Analyse exploratoire des données
  
```{r}
dim(df)
```

Nous avons donc un dataframe de 5937 lignes et de 2 colonnes. 
Nous avons donc un total de 5967 textes associés à des émotions.
Dont les noms des colonnes sont `Comment`et `Emotion` : 
```{r}
colnames(df)
```

Nous remarquons qu'il n'y a pas d'éléments dupliqués : 
```{r}
sum(duplicated(df))
```

Nous requons aussi qu'il n'y a pas d'éléments nuls
```{r}
sum(is.na(df))
```

#### Regardons la colonne `Comment`, les textes :
```{r}
df$Comment[500:503]
``` 

Nous avons donc des textes écrits en anglais avec des mots qui semblent être 
récurrents, tel que `feel` et ce qui tourne autour des sentiments.
```{r}
nchar_df <- data.frame(length = nchar(df$Comment))

plot <- ggplot(nchar_df, aes(x = length)) + geom_histogram()

plot
```

Nous remarquons avec la distribution que nous ne disposons probablement
pas de textes avec des longueurs aberrantes.

#### Regardons la colonne `Emotion`, les émotions :

```{r}
emotion <- factor(df$Emotion)
levels(emotion)
```
nous avons donc 3 émotions de diposibles qui sont :
`anger`, `fear`et `joy`.

Créons un graphique qui rend plus visuel la répartition des émotions : 
```{r}
pie_chart <- function(emotion) {
  emotion_df <- data.frame(
    Count = table(emotion)
  )
  colnames(emotion_df) <- c('Emotion', 'Count')

  emotion_df <- emotion_df %>%
                  mutate(proportion = Count / sum(emotion_df$Count) * 100) %>%
                  mutate(ypos = cumsum(proportion)- 0.5*proportion)

  pie_chart <- ggplot(emotion_df, aes(x='', y=proportion, fill=Emotion)) +
                geom_bar(stat='identity', width=1) +
                coord_polar('y', start=0) +
                theme(legend.position='none') +
                geom_text(aes(x ='', y=ypos, label=Emotion), color='white', size = 10)

  pie_chart
}

pie_chart(emotion)

```
Nous remarquons que la répartition des 3 émotions est quasiment identiques.

```{r}
nchar_df <- data.frame(length = nchar(df$Comment), Emotion = df$Emotion)

plot <- ggplot(nchar_df, aes(x=length, fill=Emotion)) +
          geom_histogram(position='identity') + 
          facet_wrap(~Emotion, scales='free')

plot
```

Nous remarquons que la répartition des longeurs des chaînes de caractères
sont similaires en fonction des émotions

### Pourquoi l'utilisation de Naive Bayésienne

## 3. Prétraitement des données

### Suppression des caractères spéciaux et chiffres
```{r}
# On mets en minuscule nos commentaires (au cas où)
df$Comment <- str_to_lower(df$Comment)

nbr_comment_false_char <- sum( ! str_detect(df$Comment, '[[:alnum:]]') == TRUE)
nbr_comment_num <- sum(str_detect(df$Comment, '^[0-9]') == TRUE)
nbr_comment_with_punct <- sum(str_detect(df$Comment, '[[:punct:]]') == TRUE)

cat('Nombre de caractères non alphanumériques :', nbr_comment_false_char,'\n')
cat('Nombre de chiffres :', nbr_comment_num,'\n')
cat('Nombre de caractères de ponctuation :', nbr_comment_with_punct,'\n')
```

__Explication :__

Nous avons 0 caractères non alphanumériques, 0 chiffres et 0 de ponctuations.
Les caractères sont tous en minuscules, nous pouvons donc regarder si il y a des mots vides 
et appliquer la tokenization. 

### Tokenization, stemming ou lemmatization

#### Tokenization

https://smltar.com/tokenization

Nous décidons dans un premier temps de faire une tokenization par mots car c est la méthode la 
plus commune et que cela nous parait logique de travailler dans un premier temps avec les mots.
```{r}
df$tokenize_1words <- tokenize_words(df$Comment)
df$tokenize_2words <- tokenize_ngrams(df$Comment, n=2)
df$tokenize_3words <- tokenize_ngrams(df$Comment, n=3)
df$tokenize_2words_min <- tokenize_ngrams(df$Comment, n=2, n_min=1)
df$tokenize_3words_min <- tokenize_ngrams(df$Comment, n=3, n_min=1)
head(df$tokenize_1words)
```
Nous décidons d appliquer différents N-gram tokenization afin d évaluer plus tard les différences sur 
les performances de notre modèle.

Nous pouvons maintenant observer les tokens les plus utilisés : 

```{r}
give_most_words <- function(df_tokens, col_names, n){
  token_freq <- table(unlist(df_tokens[[col_names]]))
  token_freq_df <- data.frame(token = names(token_freq), frequency = as.numeric(token_freq))
  token_freq_df <- token_freq_df[order(-token_freq_df$frequency),]
  plot <- ggplot(token_freq_df[0:n,], aes(x=reorder(token, -frequency), y=frequency)) +
            geom_bar(stat = 'identity')
  return(list(plot=plot, df=token_freq_df))
}
```
Pour 1 mot : 
```{r}
result <- give_most_words(df[, c('tokenize_1words', 'Emotion')], 'tokenize_1words', 10)
result$plot
```

Pour 2 mots : 
```{r}
result <- give_most_words(df[, c('tokenize_2words', 'Emotion')], 'tokenize_2words', 10)
result$plot
```

Pour 3 mots : 
```{r}
result <- give_most_words(df[, c('tokenize_3words', 'Emotion')], 'tokenize_3words', 10)
result$plot
```
__Explication :__

Nous avons définit une fonction qui permet de retourner un graph des n premiers mots et longeurs
fréquences d apparition dans notre df. 

#### Regardons maintenant les stop words 

Les __stop words__ sont les mots qui n ont pas d intérêts dans le sens de la phrase.
Ansi les supprimer réduirait le nombre de feature et donc le nombre de calculs.
Nous pouvons aussi penser que réduire les mots inutiles réduirait la performance de notre futur modèle.

Pour cela nous allons utiliser une liste de stop words déjà faite,
Nous allons donc utiliser la bibliothèque via le __package SnowBallC :__ 
```{r}
head(stopwords::stopwords(source = "snowball"))
```

Nous allons dans un premier temps travailler avec notre tokenization à 1 mot 

```{r}
temp_df <- df$tokenize_1words
# Suppose que temp_df est une liste de vecteurs de mots
df$t1_snow<- lapply(temp_df, function(x) x[!(x %in% stopwords::stopwords(source = "snowball"))])
df$t1_smart<- lapply(temp_df, function(x) x[!(x %in% stopwords::stopwords(source = "smart"))])
df$t1_iso<- lapply(temp_df, function(x) x[!(x %in% stopwords::stopwords(source = "stopwords-iso"))])

cat("Nombre de mots avant l'étape des stop words : ",
sum(lengths(temp_df)))
result <- give_most_words(df[, c('tokenize_1words', 'Emotion')], 'tokenize_1words', 10)
result$plot
cat("Nombre de mots après l'étape des stop words pour t1_snow: ",
sum(lengths(df$stop1)))
result <- give_most_words(df[, c('t1_snow', 'Emotion')], 't1_snow', 10)
result$plot
cat("Nombre de mots après l'étape des stop words pour t1_smart: ",
sum(lengths(df$stop1)))
result <- give_most_words(df[, c('t1_smart', 'Emotion')], 't1_smart', 10)
result$plot
cat("Nombre de mots après l'étape des stop words pour t1_iso: ",
sum(lengths(df$stop1)))
result <- give_most_words(df[, c('t1_iso', 'Emotion')], 't1_iso', 10)
result$plot
```


#### Stemming

Le stemming permet de rapprocher les mots similaires tel que 'feel' et 'feels' en un stem qui 
est un mot de base (ex:'feel').

```{r}
snow_df <- df$t1_snow
smart_df <- df$t1_smart
iso_df <- df$t1_iso

# Applique la fonction wordStem à chaque vecteur de mots dans la liste
df$t1_snow_stem <- lapply(snow_df, function(x) wordStem(x))
df$t1_smart_stem <- lapply(smart_df, function(x) wordStem(x))
df$t1_iso_stem <- lapply(iso_df, function(x) wordStem(x))

cat("Nombre de mots avant la stematization : ",
sum(lengths(snow_df)))
result <- give_most_words(df[, c('t1_snow', 'Emotion')], 't1_snow', 10)
result$plot
cat("Nombre de mots après la stematization pour t1_snow : ",
sum(lengths(df$t1_snow_stem)))
result <- give_most_words(df[, c('t1_snow_stem', 'Emotion')], 't1_snow_stem', 10)
result$plot

cat("Nombre de mots après la stematization pour t1_smart : ",
sum(lengths(df$t1_smart_stem)))
result <- give_most_words(df[, c('t1_smart_stem', 'Emotion')], 't1_smart_stem', 10)
result$plot

cat("Nombre de mots après la stematization pour t1_iso : ",
sum(lengths(df$t1_iso_stem)))
result <- give_most_words(df[, c('t1_iso_stem', 'Emotion')], 't1_iso_stem', 10)
result$plot

```

Nous remarquons que __feel__ comporte maintenant par exemple __feel__ et __feeling__.
Cela fonctionne bien et réduit le nombre de mots différents.


#### TF-IDF

On a :
$$
\text{IDF}(t, D) = \log\left(\frac{N}{\text{df}(t)}\right)
$$
avec 

- N : le nombre documents total
- df(t) : le nombre de documents comportant le terme t

Ainsi IDF(t,D) donne le poids IDF du terme t dans les documents de D.
Cela correspond à l importance du mot dans notre corpus de documents.

```{r}
temp_df <- df$t1_iso_stem

temp_df_corpus <- Corpus(VectorSource(temp_df))

# Créer un Document-Term Matrix (DTM)
temp_df_dtm <- DocumentTermMatrix(temp_df_corpus)

# Convertir la DTM en matrice creuse (sparse matrix)
temp_df_sparse <- as.matrix(temp_df_dtm)

# TF IDF
tfidf <- weightTfIdf(temp_df_dtm)
#

tfidf
```

#### Pourquoi ce feature engineering ?

## 4. Entrainement du modèle bayésien

Définition du test set et train set : 

```{r}
set.seed(14)

# Définir le pourcentage des données d'entraînement sur le total
percentage_train <- 0.7

train_indices <- sample(1:nrow(tfidf), floor(percentage_train * nrow(tfidf)))


X_train <- as.matrix(tfidf[train_indices, ])
X_test <- as.matrix(tfidf[-train_indices, ])
y_train <- factor(df$Emotion[train_indices])
y_test <- factor(df$Emotion[-train_indices])


cat("Taille de l'ensemble d'entraînement :",
nrow(X_train), length(y_train))

cat("Taille de l'ensemble de test :",
nrow(X_test), length(y_test))
```

```{r}
# model <- naiveBayes(X_train, y_train)

# predictions <- predict(model, X_test)

# table_confusion <- table(predictions, y_test)

# accuracy <- sum(predictions == y_test) / length(y_test)
```


Création de la fonction qui teste tout le modèle à partir d un preprocessing différent : 

```{r}
test_bayesian <- function(df_temp, df){
  temp_df <- df_temp
  #print(temp_df)
  temp_df_corpus <- Corpus(VectorSource(temp_df))

  # Créer un Document-Term Matrix (DTM)
  temp_df_dtm <- DocumentTermMatrix(temp_df_corpus)

  # Convertir la DTM en matrice creuse (sparse matrix)
  temp_df_sparse <- as.matrix(temp_df_dtm)
  print(temp_df_sparse)
  #print(temp_df_sparse)
  # TF IDF
  tfidf <- weightTfIdf(temp_df_dtm)

  #print(tfidf)
  set.seed(1)

  # Définir le pourcentage des données d'entraînement sur le total
  percentage_train <- 0.7

  train_indices <- sample(1:nrow(temp_df_sparse), floor(percentage_train * nrow(temp_df_sparse)))

  X_train <- as.matrix(tfidf[train_indices, ])
  X_test <- as.matrix(tfidf[-train_indices, ])
  y_train <- factor(df$Emotion[train_indices])
  y_test <- factor(df$Emotion[-train_indices])

  model <- naiveBayes(X_train, y_train)

  predictions <- predict(model, X_train)
  #print(y_train)
  #table_confusion <- table(predictions, y_train)

  #accuracy <- sum(predictions == y_test) / length(y_test)

  #print(X_test)
  #print(predictions)
  #print(table_confusion)
  # print(accuracy)

}
```

```{r}
test_bayesian2 <- function(df_temp, df, term_count_min, seed){
  temp_df <- df_temp
  
  set.seed(seed)
  # Définir le pourcentage des données d'entraînement sur le total
  percentage_train <- 0.70

  train_indices <- sample(1:length(temp_df), floor(percentage_train * length(temp_df)))


  X_train <- as.matrix(temp_df[train_indices])
  X_test <- as.matrix(temp_df[-train_indices])
  y_train <- factor(df$Emotion[train_indices])
  y_test <- factor(df$Emotion[-train_indices])

  # Créer un itoken
  itoken_train <- itoken(X_train)
  itoken_test <- itoken(X_test)
  
  # Créer le vocabulaire
  vocabulary <- create_vocabulary(itoken_train)
  vocabulary <- prune_vocabulary(vocabulary, term_count_min)

  vectorizer <- vocab_vectorizer(vocabulary)

  dtm_train <- create_dtm(itoken_train, vectorizer)
  dtm_test <- create_dtm(itoken_test, vectorizer)

  train_matrix <- as.matrix(as(dtm_train, "CsparseMatrix"))
  test_matrix <- as.matrix(as(dtm_test, "CsparseMatrix"))

  model <- naiveBayes(train_matrix, y_train)

  predictions <- predict(model, test_matrix)

  accuracy <- sum(predictions == y_test) / length(y_test)
  confusion <- confusionMatrix(predictions, y_test)
  return(accuracy)
  #print(confusion)
}
```

```{r}
# test_bayesian2(df$tokenize_1words[1:500], df[1:500,])
# test_bayesian2(df$tokenize_2words[1:500], df[1:500,])
# test_bayesian2(df$tokenize_3words[1:500], df[1:500,])
# test_bayesian2(df$t1_snow[1:500], df[1:500,])
# test_bayesian2(df$t1_smart[1:500], df[1:500,])
# test_bayesian2(df$t1_iso[1:500], df[1:500,])
# test_bayesian2(df$t1_snow_stem[1:500], df[1:500,])
# test_bayesian2(df$t1_smart_stem[1:500], df[1:500,])


seq_seed <- seq(1, 100, by = 10)
seq_min <- seq(5, 25, by = 5)
accuracy_values <- c()
for (j in seq_min){
  print(j)
  for(i in seq_seed){
    accuracy <- test_bayesian2(df$t1_smart_stem, df, j, i)
    #
    print(accuracy)
    accuracy_values <- c(accuracy_values, accuracy)
  } 
  print(mean(accuracy)) 
}

```


#### Pourquoi ce feature engineering ?

## 4. Entrainement du modèle bayésien


## 5. Evaluation du modèle
### Les résultats
### Les mesures d'évaluation du modèle
### Les validations croisées effectuées 

## 6. Amélioration et optimisation
### Discussion des défis rencontrés
### Raison de la performance du modèle
### Amélioration

## 7. Conclusion et travaux à venir
### Conclusion
### Améliorations possibles


#### Pourquoi ce feature engineering ?

## 4. Entrainement du modèle bayésien