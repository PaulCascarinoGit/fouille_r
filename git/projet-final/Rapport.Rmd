---
title: "Projet final : Classification bayésienne et Analyse Factorielle Discriminante "
output:
  html_document:
    theme: readable
    toc: yes
    toc_depth: 6
    toc_float: yes
author: "Paul Cascarino et Mathis Quinio-Cosquer"
date: "Février 2023"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Classification bayésienne et Analyse Factorielle Discriminante 

## 1. Préambule

### Contexte 

De nos jours, il y a une forte montée des IA capable de comprendre ainsi que de créer du langage humain.
Ces IA sont appelés __Large Language Models (LLM)__ et sont très récemment démocratisé par
l'apparition brusque de Chat gpt et de nombreux autres.

Ces IA sont capable de générer du texte, il est difficile pour un humain de savoir si le texte est fait par une IA ou non.

Dans ce projet, nous nous intéresseront à savoir si une machine serait capable de détecter cela.


### Les objectifs de ce projet

Les objectifs de ce proet sont donc de développer un système de classification
qui utilise des __méthodes bayésienne__ ainsi que des caractéristiques extraintes par 
l'__analyse factorielle discriminante (AFD)__

Dans le but de faire la distinction entre le texte généré par l'IA et par l'Homme


## 2. Données et prétraitements

### Description du jeu de données 

#### Préparation et imports 

Dans un premier temps, nous effaçons la mémoire : 
```{r}
rm(list=ls())
```

Et faisons les chargements des packages nécessaires :

```{r}
library(ggplot2)
library(dplyr)
library(stringr)

# Permet la tokenization : https://smltar.com/tokenization#types-of-tokens 
library(tokenizers)

# Permet l'utilisation de liste pour le stop-words : https://smltar.com/stopwords#premadestopwords
library(stopwords)

# Permet la stemmatization : https://smltar.com/stemming#how-to-stem-text-in-r
library(SnowballC)

# Permet le calcul du score de lisibilité 
library(quanteda.textstats)

# Permet la séparation en donnés d'entrainements et de test
library(caret)


library(tm)
library(text2vec)
library(slam)

library(MASS)
library(e1071)


```

#### Chargement des données

Les données sont récupérés sur le site kaggle à l'adresse : <https://www.kaggle.com/competitions/llm-detect-ai-generated-text>
Nous pouvons ainsi récupérer le csv récupéré avec : 
```{r}
path_essays <- 'datasets/llm-detect-ai-generated-text/train_essays.csv'
path_prompts <- 'datasets/llm-detect-ai-generated-text/train_prompts.csv'

df_essays <- read.csv(path_essays)
df_prompts <- read.csv(path_prompts)
```

#### Description d essays : 

Voici un aperçu de nos données : 

Nous avons un dataframe __d'essays__ qui sont soit écrits par des humains
soit générés par des LLM : 
```{r}
df_essays[1,]
```

Regradons ses colonnes :
```{r}
colnames(df_essays)
```

Nous avons : 

- "id" : qui est l'unique ID pour chaque essay       

- "prompt_id" : permet d'identifier le prompt de l'essay

- "text" : le texte de l'essay    

- "generated" : 0 si généré par Homme 1 si non, est une variable catégorielle =>
```{r}
df_essays$generated <- as.factor(df_essays$generated)
```

Dans l'exemple ci dessus (la première ligne), nous avons un texte écris en anglais 
par un homme (generated  = 0)

```{r}
dim(df_essays)
```
Nous avons une liste de 1378 essays et donc de 4 colonnes cités ci-dessus

```{r}
summary(df_essays)
```

Nous remarquons que : 

- "id" : contient des caractères       

- "prompt_id" : a un min à 0.0000 et a un max à 1.0000

- "text" : le texte de l'essay    

- "generated" :  a une moyenne de 0.002177 ce qui signifierait plus
  essays générés par l'Homme (0)


Nous remarquons qu'il n'y a pas d'éléments dupliqués : 
```{r}
sum(duplicated(df_essays))
```

Nous remarquons aussi qu'il n'y a pas d'éléments nuls
```{r}
sum(is.na(df_essays))
```

Regarsons maintenant notre colonne text : 

```{r}
df_essays$text[500]
```
Nous avons de long textes écrits en anglais avec des sauts de ligne (\n) et de la ponctuation.

Regardons la répartition de la longueur des textes : 
```{r}
nchar_df <- data.frame(length = nchar(df_essays$text))
nchar_df$generated <- df_essays$generated

plot <- ggplot(nchar_df, aes(x = length, fill = factor(generated))) + geom_histogram()

plot
```
Nous remarquons que nous avons des textes de longueurs très variables.
La distribution des longueurs des textes semble ne pas donner beaucoup d'information 
autre que nes ne disposons probablement pas de textes de longueur abbérantes

Regardons la répartition des textes écrits pas les humains ou IA :
```{r}
df_essays$generated <- factor(df_essays$generated)
levels(df_essays$generated)
```
nous avons donc bien 2 levels 0 pour Homme et 1 pour IA

Créons un graphique qui rend plus visuel la répartition des émotions : 
```{r}
pie_chart <- function(generated_col) {
  generated <- data.frame(
    generated = generated_col
  )
  
  generated <- generated %>%
                  count(generated) %>%
                  mutate(proportion = n / sum(n) * 100) %>%
                  mutate(ypos = cumsum(proportion) - 0.5 * proportion)

  pie_chart <- ggplot(generated, aes(x = '', y = proportion, fill = generated)) +
                geom_bar(stat = 'identity', width = 1) +
                coord_polar('y', start = 0) +
                theme(legend.position = 'none') +
                geom_text(aes(x = '', y = ypos, label = paste0(generated, ": ", round(proportion, 1), "%")), color = 'white', size = 3)

  return(pie_chart)
}

# Exécuter la fonction avec la colonne generated
pie_chart(df_essays$generated)

```
Nous remarquons l'immense majorité des textes que nous avons sont écrits par des humains

```{r}
nchar_df <- data.frame(length = nchar(df_essays$text))
nchar_df$generated <- df_essays$generated

plot <- ggplot(nchar_df, aes(x = length, fill = factor(generated))) + geom_histogram()

plot
```
Cependant nous nous rendons bien compte que les textes écrits par des humains sont bien 
plus long que ceux écrits par des IA. La longueur du texte est peut être une feature à prendre en compte.

```{r}
df_generated_1 <- subset(df_essays, generated == 1)
dim(df_generated_1)
```

Nous n'avons que très peu de texts écrits pas l'IA dans notre dataframe.
Cela peut poser problème et donc les idées émises précédemment ne sont pas à prendre en compte
#### Description des prompts : 

Notre dataframe de promt nous permets de voir les promts données aux 
Hommes ou LLM qui on donnés lieux aux essay.

Les 2 sont liés par le "prompt_id".

```{r}
colnames(df_prompts)
```
Nous avons : 

- "prompt_id" : qui est unique pour chaque prompt

- "prompt_name" : Le titre du prompt

- "instructions" : Les instructions données aux étudiants (Humains)

- "source_text" : La source (articles) des essays pour répondre aux prompts

```{r}
dim(df_prompts)
```

Après avoir passé __BEAUCOUP__ de temps à essayer de faire fonctionner 
notre df_prompts correctement, nous nous apercevons qu'il y a beaucoup 
de valeurs manquantes et nous pensons qu'il n'est pas nécessaire aux 
méthodes que nous allons utiliser

Ainsi nous avons pour plus de simplicité
```{r}
df <- df_essays
```

### Etapes de prétraitements

#### Nettoyage des données textuelles

Nous avons 1 colonne avec des données textuelles non catégorielle : __text__

Pour nétoyer nous allons créer une fonction qui vas : 
- mettre tous les textes en minuscule
- supprimer les chiffres
- supprimer tous les caractères spéciaux
- retirer les sauts de lignes

```{r}
clean_text <- function(df_col){
  temp_col <- str_to_lower(df_col)
  temp_col <- str_replace_all(temp_col, "'", " ") 
  temp_col <- str_replace_all(temp_col, "[0-9]", "")
  temp_col <- str_replace_all(temp_col, "[^[:alnum:][:space:]]", "")
  temp_col <- str_replace_all(temp_col, "\n", "")

  return(temp_col)
}

df$tidy <- clean_text(df$text)

head(df$tidy)
```

La fonction __clean_text__ nous permet de supprimer les caractères non alphanumériques
et les chiffres ainsi que de touus mettre en minuscule. 

Nous pouvons maintenant apppliquer la Tokenization : 

#### Tokenization

https://smltar.com/tokenization

Nous décidons dans un premier temps de faire une tokenization par mots car c est la méthode la 
plus commune et que cela nous parait logique de travailler dans un premier temps avec les mots.

De plus nous avons vu grâce au mini projet 1 que c'est la plus performante par rapports 
à la tokenization par pluusieurs mots en terme d'accuracy : 

```{r}
df$token <- tokenize_words(df$tidy)

df$token[1]
```

Nous pouvons maintenant observer les tokens les plus utilisés : 

Utilisons la fonction du mini-projet 1 : 

```{r}
give_most_words <- function(df_tokens, col_names, n){
  token_freq <- table(unlist(df_tokens[[col_names]]))
  token_freq_df <- data.frame(token = names(token_freq), frequency = as.numeric(token_freq))
  token_freq_df <- token_freq_df[order(-token_freq_df$frequency),]
  plot <- ggplot(token_freq_df[0:n,], aes(x=reorder(token, -frequency), y=frequency)) +
            geom_bar(stat = 'identity')
  return(list(plot=plot, df=token_freq_df))
}
result <- give_most_words(df[, c('token', 'generated')], 'token', 10)

result$plot
```

__Explication :__

Nous avons définit une fonction qui permet de retourner un graph des n premiers mots et longeurs
fréquences d apparition dans notre df. 


Nous remarquons que nous avons une liste de mots inutiles. Regardons maintenant les stop words : 

##### Stop words

Les __stop words__ sont les mots qui n ont pas d intérêts dans le sens de la phrase.
Ansi les supprimer réduirait le nombre de feature et donc le nombre de calculs.
Nous pouvons aussi penser que réduire les mots inutiles réduirait la performance de notre futur modèle.

Pour cela nous allons utiliser une liste de stop words déjà faite,
Nous allons donc utiliser la bibliothèque __snowball__.
```{r}
head(stopwords::stopwords(source = "snowball"))
```


```{r}
cat("Nombre de mots des textes avant l'étape des stop words : ",
sum(lengths(df$token)))

df$stopwords <- sapply(df$token, function(x) setdiff(x, stopwords::stopwords(source = "snowball")))

result <- give_most_words(df[, c('stopwords', 'generated')], 'stopwords', 10)

result$plot


cat("Nombre de mots des textes après l'étape des stop words : ",
sum(lengths(df$stopwords)))

```

Nous venons de suupprimer les mots les plus inutiles qui sont dans la bibliothèque __smart__

##### Stemming 

Le stemming permet de rapprocher les mots similaires tel que 'feel' et 'feels' en un stem qui 
est un mot de base (ex:'feel').

```{r}
df$stem <- lapply(df$stopwords, function(x) wordStem(x))

result <- give_most_words(df[, c('stem', 'generated')], 'stem', 10)

result$plot

```

Nous remarquons bien la différence après l étape du stemming et l appartition de nouveaux mots.


## 3. Extraction de caractéristiques (Feature Extraction)

### Caractéristiques linguistiques et stylométriques

Regardons certaines caractéristiques qui pourraient différencier les écrits humains aux IA : 

#### Calcul du nombre de phrase

Pour cela nous splitons toous nos textes en phrase avec : 
```{r}
df$phrases <- lapply(strsplit(df$text, "[.!?]\\s|\n"), unlist)

# Suppression des éléments vides
df$phrases <- lapply(df$phrases, function(x) Filter(nzchar, x))

head(df$phrases[1])
```

Ainsi il est simple d'avoir le nombre de phrase par essay avec 

```{r}
df$p_nbr <- sapply(df$phrases, length)
head(df$p_nbr)
cat('Nombre moyennes des phrases dans les essays : ',mean(df$p_nbr))
```

#### Calcul de la moyenne des longueurs des phrases

```{r}
df$p_moy_chars <- sapply(df$phrases, function(phrases) mean(nchar(phrases)))
head(df$p_moy_chars)
cat('Nombre moyennes des caractères par phrases ', mean(df$p_moy_chars))
```

#### Calcul des scores de lisibilités

Nous pouvons aussi calculer le score de lisibilité avec la package __quanteda.textstats__.
Cela peut peut être nous donner une information quant à la différences des écrits humain et des IA
```{r}
df$read_score <- textstat_readability(df$text)$Flesch
head(df$read_score)
cat('Nombre moyennes des scores de lisibilité ', mean(df$read_score))
```

#### Regard sur les caractères non alphanumérique

Nous voullons aussi regarder si la ponctuation et le nombre de caractères utilisés nous
donnerai des information quant à la différences des écrits humain et des IA.

Nous pouvons ainsi compter le nombre de caractères émis : 
Nous utiliserons la librairie __stringr__ pour plus de simplicité

```{r}
df$ponct_count <- str_count(df$text, "[[:punct:]]")
head(df$ponct_count)
cat('Nombre moyennes des ponctuations écrites ', mean(df$ponct_count))
```

Nous pouvons aussi faire un ration ponctuation / nombre de caractère



```{r}
df$ponct_ratio <- df$ponct_count / nchar(df$text)
head(df$ponct_ratio)
cat('Nombre moyennes des ponctuations / caractères ', mean(df$ponct_ratio))
```

### Technique de vectorisation utilisée

#### Explication

Nous décidons d'effectuer une vectorisation par TF IDF car : 

- Cela permet de réduire la dimensionnalité de notre texte
 
- Identifier les termes important et ainsi pouvoir reconnaitre les redondances des IA par exemples

- Réduire le bruits, les mots peu utilisés et non utiles

On a :
 $$
\text{IDF}(t, D) = \log\left(\frac{N}{\text{df}(t)}\right)
$$
avec 

- N : le nombre documents total
- df(t) : le nombre de documents comportant le terme t

Ainsi IDF(t,D) donne le poids IDF du terme t dans les documents de D.
Cela correspond à l importance du mot dans notre corpus de documents.

#### Suréchantillonnage

Nous n avons pas beaucoup de données écrites par les IA, nous pouvons dupliquer leurs lignes 
afin d'apporter un plus gros poids à celles ci.

```{r}
dim(df)


set.seed(1)

df_H <- df[df$generated == 0, ]
df_IA <- df[df$generated == 1, ]

# Calcul du facteur de duplication
n <- nrow(df_H) / nrow(df_IA)


df_IA_plus <- df_IA[rep(1:nrow(df_IA), times = ceiling(n)), ]

df_plus <- rbind(df_H, df_IA_plus)
dim(df_plus)

```


#### Séparation en données d'entrainements et de test
```{r}
set.seed(1)

pourcentage <- 0.7

indices <- createDataPartition(df_plus$generated, p = pourcentage, list = FALSE)

# df_train <- df_plus[indices, ]
# df_test <- df_plus[-indices, ]

df_train <- df_plus[indices, ]
df_test <- df_plus[-indices, ]


# Afficher la taille des ensembles d'entraînement et de test
cat("Taille de l'ensemble d'entraînement :", nrow(df_train), "\n")
cat("Taille de l'ensemble de test :", nrow(df_test), "\n")

dim(df_train[df_train$generated == 1,])
dim(df_test[df_test$generated == 1,])
```

#### Calculs de notre TF IDF
```{r}
give_tfidf <- function(df_train, df_test, term_count_min){

    # Créer un itoken
  itoken_train <- itoken(df_train$stem)
  itoken_test <- itoken(df_test$stem)
  
  # Créer le vocabulaire
  vocabulary <- create_vocabulary(itoken_train)
  vocabulary <- prune_vocabulary(vocabulary, term_count_min)

  vectorizer <- vocab_vectorizer(vocabulary)

  dtm_train <- create_dtm(itoken_train, vectorizer)
  dtm_test <- create_dtm(itoken_test, vectorizer)

  train_matrix <- as.matrix(as(dtm_train, "CsparseMatrix"))
  test_matrix <- as.matrix(as(dtm_test, "CsparseMatrix"))

  return(list(train = train_matrix, test = test_matrix))
}
tfidfs <- give_tfidf(df_train, df_test, 5)

df_train$tfidf <- tfidfs$train
df_test$tfidf <- tfidfs$test
```


## 4. Analyse des facteurs discriminants

### Explication de l AFD

#### Résumé de nos colonnes

```{r}
colnames(df_train)
```
Nous avons un total de 14 colonnes pour l'instant. 

Seulement de nombreuses sont la modifications d'autres réduisons cela popur éviter la redondances

```{r}
df_train_afd <- subset(df_train, select = c("generated", "p_nbr", "p_moy_chars", "read_score", "ponct_count", "ponct_ratio" ))
df_test_afd <- subset(df_test, select = c("generated", "p_nbr", "p_moy_chars", "read_score", "ponct_count", "ponct_ratio" ))
# Créer le sous-ensemble de colonnes sans la colonne tfidf et afficher le résumé statistique
summary(df_train_afd[, c("generated", "p_nbr", "p_moy_chars", "read_score", "ponct_count", "ponct_ratio")])

```

Explication de l AFD, y compris les fondements mathématiques et l application à 
l ensemble de données

```{r}
# Ajuster le modèle LDA sans spécifier le nombre de composantes
model_lda <- lda(generated ~ ., data = df_train_afd)

# Obtenir le nombre de composantes générées par le modèle
num_components <- ncol(predict(model_lda)$x)

# Utiliser les résultats du modèle LDA pour sélectionner toutes les composantes discriminantes
selected_lda_components <- predict(model_lda)$x[, 1:num_components]

# Ajouter la variable cible au nouveau jeu de données
df_selected_components <- data.frame(selected_lda_components, generated = df_train_afd$generated)

# Résumé des composantes discriminantes sélectionnées
head(df_selected_components)
```
```{r}
df_train_afd_with_tfidf <- cbind(df_train_afd, tfidf = df_train$tfidf)

model_bayes <- naiveBayes(generated ~ ., data = df_train_afd_with_tfidf)
summary(model_bayes)



predictions <- predict(model_bayes, newdata = df_test_afd)

accuracy <- sum(predictions == df_test_afd$generated) / length(df_test_afd$generated)
accuracy
```


## 5. Classification bayésienne

Élaborer sur la méthodologie de classification bayésienne utilisée, y compris les 
techniques avancées

## 6. Evaluation et résultats

Discutez des mesures d évaluation du modèle et du processus de validation.

## 7. Conclusion

Résumer les principales constatations et conclusions tirées de l analyse

## 8. Travaux futurs

Mettez en évidence les domaines nécessitant des recherches plus approfondies, tels que 
l amélioration du modèle ou l exploration de fonctionnalités alternatives. 